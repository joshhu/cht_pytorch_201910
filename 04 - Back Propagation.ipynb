{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 4.0\n",
      "\tgrad:  1.0 2.0 tensor(-2.)\n",
      "\tgrad:  2.0 4.0 tensor(-9.8400)\n",
      "\tgrad:  3.0 6.0 tensor(-25.7088)\n",
      "progress: 0 tensor(6.9950)\n",
      "\tgrad:  1.0 2.0 tensor(-1.2490)\n",
      "\tgrad:  2.0 4.0 tensor(-6.1452)\n",
      "\tgrad:  3.0 6.0 tensor(-16.0555)\n",
      "progress: 1 tensor(2.7281)\n",
      "\tgrad:  1.0 2.0 tensor(-0.7800)\n",
      "\tgrad:  2.0 4.0 tensor(-3.8377)\n",
      "\tgrad:  3.0 6.0 tensor(-10.0268)\n",
      "progress: 2 tensor(1.0640)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4871)\n",
      "\tgrad:  2.0 4.0 tensor(-2.3967)\n",
      "\tgrad:  3.0 6.0 tensor(-6.2619)\n",
      "progress: 3 tensor(0.4150)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3042)\n",
      "\tgrad:  2.0 4.0 tensor(-1.4968)\n",
      "\tgrad:  3.0 6.0 tensor(-3.9106)\n",
      "progress: 4 tensor(0.1618)\n",
      "\tgrad:  1.0 2.0 tensor(-0.1900)\n",
      "\tgrad:  2.0 4.0 tensor(-0.9348)\n",
      "\tgrad:  3.0 6.0 tensor(-2.4422)\n",
      "progress: 5 tensor(0.0631)\n",
      "\tgrad:  1.0 2.0 tensor(-0.1187)\n",
      "\tgrad:  2.0 4.0 tensor(-0.5838)\n",
      "\tgrad:  3.0 6.0 tensor(-1.5252)\n",
      "progress: 6 tensor(0.0246)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0741)\n",
      "\tgrad:  2.0 4.0 tensor(-0.3646)\n",
      "\tgrad:  3.0 6.0 tensor(-0.9525)\n",
      "progress: 7 tensor(0.0096)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0463)\n",
      "\tgrad:  2.0 4.0 tensor(-0.2277)\n",
      "\tgrad:  3.0 6.0 tensor(-0.5949)\n",
      "progress: 8 tensor(0.0037)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0289)\n",
      "\tgrad:  2.0 4.0 tensor(-0.1422)\n",
      "\tgrad:  3.0 6.0 tensor(-0.3715)\n",
      "progress: 9 tensor(0.0015)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0180)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0888)\n",
      "\tgrad:  3.0 6.0 tensor(-0.2320)\n",
      "progress: 10 tensor(0.0006)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0113)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0555)\n",
      "\tgrad:  3.0 6.0 tensor(-0.1449)\n",
      "progress: 11 tensor(0.0002)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0070)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0346)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0905)\n",
      "progress: 12 tensor(8.6654e-05)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0044)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0216)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0565)\n",
      "progress: 13 tensor(3.3798e-05)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0027)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0135)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0353)\n",
      "progress: 14 tensor(1.3185e-05)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0017)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0084)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0220)\n",
      "progress: 15 tensor(5.1409e-06)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0011)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0053)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0138)\n",
      "progress: 16 tensor(2.0043e-06)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0007)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0033)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0086)\n",
      "progress: 17 tensor(7.8240e-07)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0004)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0021)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0054)\n",
      "progress: 18 tensor(3.0543e-07)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0003)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0013)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0034)\n",
      "progress: 19 tensor(1.1885e-07)\n",
      "predict (after training) 4 tensor(7.9997)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = torch.tensor([1.0],  requires_grad=True)  # Any random value\n",
    "\n",
    "# our model forward pass\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "# Loss function\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\", 4, forward(4).item())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, y_val)\n",
    "        l.backward()\n",
    "        print(\"\\tgrad: \", x_val, y_val, w.grad.data[0])\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "    w.grad.data.zero_()\n",
    "\n",
    "    print(\"progress:\", epoch, l.data[0])\n",
    "\n",
    "# After training\n",
    "print(\"predict (after training)\", 4, forward(4).data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 tensor(21.)\n",
      "\tgrad:  1.0 2.0 tensor(2.) tensor(2.) tensor(2.)\n",
      "\tgrad:  2.0 4.0 tensor(14.) tensor(26.) tensor(8.)\n",
      "\tgrad:  3.0 6.0 tensor(56.) tensor(152.) tensor(22.)\n",
      "progress: 0 tensor(49.)\n",
      "\tgrad:  1.0 2.0 tensor(-2.6000) tensor(-2.6000) tensor(-2.6000)\n",
      "\tgrad:  2.0 4.0 tensor(-20.2800) tensor(-37.9600) tensor(-11.4400)\n",
      "\tgrad:  3.0 6.0 tensor(-71.7600) tensor(-192.4000) tensor(-28.6000)\n",
      "progress: 1 tensor(73.6164)\n",
      "\tgrad:  1.0 2.0 tensor(3.2552) tensor(3.2552) tensor(3.2552)\n",
      "\tgrad:  2.0 4.0 tensor(23.2440) tensor(43.2328) tensor(13.2496)\n",
      "\tgrad:  3.0 6.0 tensor(90.2928) tensor(244.3792) tensor(35.5992)\n",
      "progress: 2 tensor(124.8761)\n",
      "\tgrad:  1.0 2.0 tensor(-4.1502) tensor(-4.1502) tensor(-4.1502)\n",
      "\tgrad:  2.0 4.0 tensor(-31.9095) tensor(-59.6688) tensor(-18.0299)\n",
      "\tgrad:  3.0 6.0 tensor(-115.2141) tensor(-309.5826) tensor(-45.7981)\n",
      "progress: 3 tensor(192.7683)\n",
      "\tgrad:  1.0 2.0 tensor(5.2617) tensor(5.2617) tensor(5.2617)\n",
      "\tgrad:  2.0 4.0 tensor(38.0847) tensor(70.9077) tensor(21.6732)\n",
      "\tgrad:  3.0 6.0 tensor(145.4411) tensor(392.9769) tensor(57.4586)\n",
      "progress: 4 tensor(320.1500)\n",
      "\tgrad:  1.0 2.0 tensor(-6.6559) tensor(-6.6559) tensor(-6.6559)\n",
      "\tgrad:  2.0 4.0 tensor(-50.6428) tensor(-94.6297) tensor(-28.6493)\n",
      "\tgrad:  3.0 6.0 tensor(-185.1208) tensor(-498.0638) tensor(-73.4753)\n",
      "progress: 5 tensor(502.3427)\n",
      "\tgrad:  1.0 2.0 tensor(8.4773) tensor(8.4773) tensor(8.4773)\n",
      "\tgrad:  2.0 4.0 tensor(61.9293) tensor(115.3813) tensor(35.2033)\n",
      "\tgrad:  3.0 6.0 tensor(234.1360) tensor(632.0013) tensor(92.6055)\n",
      "progress: 6 tensor(823.7540)\n",
      "\tgrad:  1.0 2.0 tensor(-10.6975) tensor(-10.6975) tensor(-10.6975)\n",
      "\tgrad:  2.0 4.0 tensor(-80.8009) tensor(-150.9043) tensor(-45.7492)\n",
      "\tgrad:  3.0 6.0 tensor(-297.5757) tensor(-801.2288) tensor(-118.0075)\n",
      "progress: 7 tensor(1305.3149)\n",
      "\tgrad:  1.0 2.0 tensor(13.6387) tensor(13.6387) tensor(13.6387)\n",
      "\tgrad:  2.0 4.0 tensor(100.2583) tensor(186.8779) tensor(56.9485)\n",
      "\tgrad:  3.0 6.0 tensor(376.7911) tensor(1016.4764) tensor(149.1261)\n",
      "progress: 8 tensor(2124.1777)\n",
      "\tgrad:  1.0 2.0 tensor(-17.2092) tensor(-17.2092) tensor(-17.2092)\n",
      "\tgrad:  2.0 4.0 tensor(-129.3341) tensor(-241.4590) tensor(-73.2716)\n",
      "\tgrad:  3.0 6.0 tensor(-478.4685) tensor(-1288.8622) tensor(-189.6497)\n",
      "progress: 9 tensor(3385.9675)\n",
      "\tgrad:  1.0 2.0 tensor(21.9305) tensor(21.9305) tensor(21.9305)\n",
      "\tgrad:  2.0 4.0 tensor(161.8869) tensor(301.8434) tensor(91.9087)\n",
      "\tgrad:  3.0 6.0 tensor(606.2415) tensor(1634.9069) tensor(240.0269)\n",
      "progress: 10 tensor(5484.7476)\n",
      "\tgrad:  1.0 2.0 tensor(-27.6930) tensor(-27.6930) tensor(-27.6930)\n",
      "\tgrad:  2.0 4.0 tensor(-207.4220) tensor(-387.1511) tensor(-117.5575)\n",
      "\tgrad:  3.0 6.0 tensor(-769.4423) tensor(-2073.2119) tensor(-304.8976)\n",
      "progress: 11 tensor(8774.0771)\n",
      "\tgrad:  1.0 2.0 tensor(35.2580) tensor(35.2580) tensor(35.2580)\n",
      "\tgrad:  2.0 4.0 tensor(260.9942) tensor(486.7304) tensor(148.1261)\n",
      "\tgrad:  3.0 6.0 tensor(975.3018) tensor(2629.6533) tensor(386.2286)\n",
      "progress: 12 tensor(14173.2051)\n",
      "\tgrad:  1.0 2.0 tensor(-44.5657) tensor(-44.5657) tensor(-44.5657)\n",
      "\tgrad:  2.0 4.0 tensor(-333.0473) tensor(-621.5289) tensor(-188.8065)\n",
      "\tgrad:  3.0 6.0 tensor(-1237.4805) tensor(-3334.8284) tensor(-490.2842)\n",
      "progress: 13 tensor(22722.2051)\n",
      "\tgrad:  1.0 2.0 tensor(56.6862) tensor(56.6862) tensor(56.6862)\n",
      "\tgrad:  2.0 4.0 tensor(420.3868) tensor(784.0875) tensor(238.5365)\n",
      "\tgrad:  3.0 6.0 tensor(1568.9244) tensor(4229.7007) tensor(621.3824)\n",
      "progress: 14 tensor(36642.7422)\n",
      "\tgrad:  1.0 2.0 tensor(-71.7140) tensor(-71.7140) tensor(-71.7140)\n",
      "\tgrad:  2.0 4.0 tensor(-535.1346) tensor(-998.5552) tensor(-303.4243)\n",
      "\tgrad:  3.0 6.0 tensor(-1990.3247) tensor(-5364.1255) tensor(-788.4877)\n",
      "progress: 15 tensor(58821.6133)\n",
      "\tgrad:  1.0 2.0 tensor(91.1448) tensor(91.1448) tensor(91.1448)\n",
      "\tgrad:  2.0 4.0 tensor(676.7497) tensor(1262.3546) tensor(383.9472)\n",
      "\tgrad:  3.0 6.0 tensor(2523.7551) tensor(6803.3706) tensor(999.6157)\n",
      "progress: 16 tensor(94761.9141)\n",
      "\tgrad:  1.0 2.0 tensor(-115.3900) tensor(-115.3900) tensor(-115.3900)\n",
      "\tgrad:  2.0 4.0 tensor(-860.2095) tensor(-1605.0288) tensor(-487.7997)\n",
      "\tgrad:  3.0 6.0 tensor(-3201.2771) tensor(-8628.2324) tensor(-1268.1556)\n",
      "progress: 17 tensor(152238.8281)\n",
      "\tgrad:  1.0 2.0 tensor(146.5633) tensor(146.5633) tensor(146.5633)\n",
      "\tgrad:  2.0 4.0 tensor(1089.0895) tensor(2031.6156) tensor(617.8264)\n",
      "\tgrad:  3.0 6.0 tensor(4059.5869) tensor(10943.1074) tensor(1607.9922)\n",
      "progress: 18 tensor(245107.0625)\n",
      "\tgrad:  1.0 2.0 tensor(-185.6505) tensor(-185.6505) tensor(-185.6505)\n",
      "\tgrad:  2.0 4.0 tensor(-1383.1079) tensor(-2580.5654) tensor(-784.3792)\n",
      "\tgrad:  3.0 6.0 tensor(-5149.0938) tensor(-13878.5225) tensor(-2039.7078)\n",
      "progress: 19 tensor(393962.4688)\n",
      "\tgrad:  1.0 2.0 tensor(235.6960) tensor(235.6960) tensor(235.6960)\n",
      "\tgrad:  2.0 4.0 tensor(1752.3177) tensor(3268.9395) tensor(994.0068)\n",
      "\tgrad:  3.0 6.0 tensor(6529.9536) tensor(17601.8457) tensor(2586.5520)\n",
      "progress: 20 tensor(634050.)\n",
      "\tgrad:  1.0 2.0 tensor(-298.6710) tensor(-298.6710) tensor(-298.6710)\n",
      "\tgrad:  2.0 4.0 tensor(-2224.2026) tensor(-4149.7344) tensor(-1261.4369)\n",
      "\tgrad:  3.0 6.0 tensor(-8282.1484) tensor(-22323.5703) tensor(-3280.7520)\n",
      "progress: 21 tensor(1019408.4375)\n",
      "\tgrad:  1.0 2.0 tensor(379.0584) tensor(379.0584) tensor(379.0584)\n",
      "\tgrad:  2.0 4.0 tensor(2819.0999) tensor(5259.1416) tensor(1599.0791)\n",
      "\tgrad:  3.0 6.0 tensor(10503.5137) tensor(28312.3828) tensor(4160.5508)\n",
      "progress: 22 tensor(1640284.)\n",
      "\tgrad:  1.0 2.0 tensor(-480.4705) tensor(-480.4705) tensor(-480.4705)\n",
      "\tgrad:  2.0 4.0 tensor(-3577.1133) tensor(-6673.7563) tensor(-2028.7920)\n",
      "\tgrad:  3.0 6.0 tensor(-13321.6504) tensor(-35907.3711) tensor(-5276.9712)\n",
      "progress: 23 tensor(2637667.)\n",
      "\tgrad:  1.0 2.0 tensor(609.6493) tensor(609.6493) tensor(609.6493)\n",
      "\tgrad:  2.0 4.0 tensor(4534.9966) tensor(8460.3438) tensor(2572.3230)\n",
      "\tgrad:  3.0 6.0 tensor(16894.9531) tensor(45540.2148) tensor(6692.3086)\n",
      "progress: 24 tensor(4243571.)\n",
      "\tgrad:  1.0 2.0 tensor(-772.9001) tensor(-772.9001) tensor(-772.9001)\n",
      "\tgrad:  2.0 4.0 tensor(-5753.2754) tensor(-10733.6504) tensor(-3263.0876)\n",
      "\tgrad:  3.0 6.0 tensor(-21427.6621) tensor(-57756.8125) tensor(-8487.8828)\n",
      "progress: 25 tensor(6824622.)\n",
      "\tgrad:  1.0 2.0 tensor(980.5470) tensor(980.5470) tensor(980.5470)\n",
      "\tgrad:  2.0 4.0 tensor(7294.9897) tensor(13609.4326) tensor(4137.7686)\n",
      "\tgrad:  3.0 6.0 tensor(27175.5332) tensor(73251.0625) tensor(10764.6162)\n",
      "progress: 26 tensor(10978777.)\n",
      "\tgrad:  1.0 2.0 tensor(-1243.2773) tensor(-1243.2773) tensor(-1243.2773)\n",
      "\tgrad:  2.0 4.0 tensor(-9253.6318) tensor(-17263.9863) tensor(-5248.4546)\n",
      "\tgrad:  3.0 6.0 tensor(-34466.1367) tensor(-92901.5000) tensor(-13652.6230)\n",
      "progress: 27 tensor(17657510.)\n",
      "\tgrad:  1.0 2.0 tensor(1577.1278) tensor(1577.1278) tensor(1577.1278)\n",
      "\tgrad:  2.0 4.0 tensor(11734.4082) tensor(21891.6875) tensor(6655.7681)\n",
      "\tgrad:  3.0 6.0 tensor(43711.7734) tensor(117823.7891) tensor(17314.8906)\n",
      "progress: 28 tensor(28404220.)\n",
      "\tgrad:  1.0 2.0 tensor(-1999.8813) tensor(-1999.8813) tensor(-1999.8813)\n",
      "\tgrad:  2.0 4.0 tensor(-14883.9453) tensor(-27768.0098) tensor(-8441.9141)\n",
      "\tgrad:  3.0 6.0 tensor(-55438.4375) tensor(-149431.4844) tensor(-21960.0781)\n",
      "progress: 29 tensor(45685188.)\n",
      "\tgrad:  1.0 2.0 tensor(2536.7185) tensor(2536.7185) tensor(2536.7185)\n",
      "\tgrad:  2.0 4.0 tensor(18875.1699) tensor(35213.6211) tensor(10705.9443)\n",
      "\tgrad:  3.0 6.0 tensor(70310.2031) tensor(189518.7188) tensor(27850.9531)\n",
      "progress: 30 tensor(73487840.)\n",
      "\tgrad:  1.0 2.0 tensor(-3216.8789) tensor(-3216.8789) tensor(-3216.8789)\n",
      "\tgrad:  2.0 4.0 tensor(-23940.2754) tensor(-44663.6719) tensor(-13578.5771)\n",
      "\tgrad:  3.0 6.0 tensor(-89172.2422) tensor(-240359.5781) tensor(-35322.5664)\n",
      "progress: 31 tensor(1.1820e+08)\n",
      "\tgrad:  1.0 2.0 tensor(4080.2087) tensor(4080.2087) tensor(4080.2087)\n",
      "\tgrad:  2.0 4.0 tensor(30361.0254) tensor(56641.8398) tensor(17220.6172)\n",
      "\tgrad:  3.0 6.0 tensor(113093.5781) tensor(304839.5000) tensor(44798.1328)\n",
      "progress: 32 tensor(1.9013e+08)\n",
      "\tgrad:  1.0 2.0 tensor(-5174.4155) tensor(-5174.4155) tensor(-5174.4155)\n",
      "\tgrad:  2.0 4.0 tensor(-38507.3320) tensor(-71840.2500) tensor(-21840.8750)\n",
      "\tgrad:  3.0 6.0 tensor(-143432.8281) tensor(-386616.7500) tensor(-56816.0430)\n",
      "progress: 33 tensor(3.0582e+08)\n",
      "\tgrad:  1.0 2.0 tensor(6562.8970) tensor(6562.8970) tensor(6562.8970)\n",
      "\tgrad:  2.0 4.0 tensor(48835.9297) tensor(91108.9609) tensor(27699.4121)\n",
      "\tgrad:  3.0 6.0 tensor(181910.3438) tensor(490332.1562) tensor(72057.5469)\n",
      "progress: 34 tensor(4.9191e+08)\n",
      "\tgrad:  1.0 2.0 tensor(-8323.1035) tensor(-8323.1035) tensor(-8323.1035)\n",
      "\tgrad:  2.0 4.0 tensor(-61938.3438) tensor(-115553.5781) tensor(-35130.7227)\n",
      "\tgrad:  3.0 6.0 tensor(-230710.5938) tensor(-621870.3125) tensor(-91388.1406)\n",
      "progress: 35 tensor(7.9122e+08)\n",
      "\tgrad:  1.0 2.0 tensor(10556.2773) tensor(10556.2773) tensor(10556.2773)\n",
      "\tgrad:  2.0 4.0 tensor(78552.6562) tensor(146549.0469) tensor(44554.4688)\n",
      "\tgrad:  3.0 6.0 tensor(292601.5625) tensor(788695.8125) tensor(115904.1094)\n",
      "progress: 36 tensor(1.2727e+09)\n",
      "\tgrad:  1.0 2.0 tensor(-13387.7520) tensor(-13387.7520) tensor(-13387.7520)\n",
      "\tgrad:  2.0 4.0 tensor(-99626.9844) tensor(-185866.2188) tensor(-56507.3672)\n",
      "\tgrad:  3.0 6.0 tensor(-371096.3750) tensor(-1000274.3750) tensor(-146997.1562)\n",
      "progress: 37 tensor(2.0471e+09)\n",
      "\tgrad:  1.0 2.0 tensor(16979.6035) tensor(16979.6035) tensor(16979.6035)\n",
      "\tgrad:  2.0 4.0 tensor(126351.8516) tensor(235724.1094) tensor(71665.7266)\n",
      "\tgrad:  3.0 6.0 tensor(470647.7812) tensor(1268611.8750) tensor(186431.0312)\n",
      "progress: 38 tensor(3.2928e+09)\n",
      "\tgrad:  1.0 2.0 tensor(-21534.2090) tensor(-21534.2090) tensor(-21534.2090)\n",
      "\tgrad:  2.0 4.0 tensor(-160248.9062) tensor(-298963.6250) tensor(-90891.5625)\n",
      "\tgrad:  3.0 6.0 tensor(-596905.8750) tensor(-1608934.3750) tensor(-236443.8750)\n",
      "progress: 39 tensor(5.2964e+09)\n",
      "\tgrad:  1.0 2.0 tensor(27311.4727) tensor(27311.4727) tensor(27311.4727)\n",
      "\tgrad:  2.0 4.0 tensor(203236.4844) tensor(379161.5000) tensor(115273.9844)\n",
      "\tgrad:  3.0 6.0 tensor(757033.8750) tensor(2040553.6250) tensor(299873.1250)\n",
      "progress: 40 tensor(8.5192e+09)\n",
      "\tgrad:  1.0 2.0 tensor(-34637.7344) tensor(-34637.7344) tensor(-34637.7344)\n",
      "\tgrad:  2.0 4.0 tensor(-257758.9062) tensor(-480880.0625) tensor(-146198.3125)\n",
      "\tgrad:  3.0 6.0 tensor(-960118.8750) tensor(-2587960.) tensor(-380318.3125)\n",
      "progress: 41 tensor(1.3703e+10)\n",
      "\tgrad:  1.0 2.0 tensor(43930.2109) tensor(43930.2109) tensor(43930.2109)\n",
      "\tgrad:  2.0 4.0 tensor(326904.8750) tensor(609879.5000) tensor(185417.5312)\n",
      "\tgrad:  3.0 6.0 tensor(1217683.7500) tensor(3282216.) tensor(482343.8125)\n",
      "progress: 42 tensor(2.2041e+10)\n",
      "\tgrad:  1.0 2.0 tensor(-55714.6641) tensor(-55714.6641) tensor(-55714.6641)\n",
      "\tgrad:  2.0 4.0 tensor(-414603.0312) tensor(-773491.4375) tensor(-235158.8438)\n",
      "\tgrad:  3.0 6.0 tensor(-1544344.5000) tensor(-4162716.) tensor(-611739.3750)\n",
      "progress: 43 tensor(3.5453e+10)\n",
      "\tgrad:  1.0 2.0 tensor(70661.3281) tensor(70661.3281) tensor(70661.3281)\n",
      "\tgrad:  2.0 4.0 tensor(525824.6875) tensor(980988.) tensor(298243.)\n",
      "\tgrad:  3.0 6.0 tensor(1958636.2500) tensor(5279422.5000) tensor(775846.8125)\n",
      "progress: 44 tensor(5.7026e+10)\n",
      "\tgrad:  1.0 2.0 tensor(-89616.7656) tensor(-89616.7656) tensor(-89616.7656)\n",
      "\tgrad:  2.0 4.0 tensor(-666885.7500) tensor(-1244154.7500) tensor(-378251.2500)\n",
      "\tgrad:  3.0 6.0 tensor(-2484067.5000) tensor(-6695700.) tensor(-983978.5000)\n",
      "progress: 45 tensor(9.1726e+10)\n",
      "\tgrad:  1.0 2.0 tensor(113658.1484) tensor(113658.1484) tensor(113658.1484)\n",
      "\tgrad:  2.0 4.0 tensor(845785.6875) tensor(1577913.2500) tensor(479721.9375)\n",
      "\tgrad:  3.0 6.0 tensor(3150452.7500) tensor(8491915.) tensor(1247944.2500)\n",
      "progress: 46 tensor(1.4754e+11)\n",
      "\tgrad:  1.0 2.0 tensor(-144148.0938) tensor(-144148.0938) tensor(-144148.0938)\n",
      "\tgrad:  2.0 4.0 tensor(-1072681.) tensor(-2001213.8750) tensor(-608414.5000)\n",
      "\tgrad:  3.0 6.0 tensor(-3995606.) tensor(-10769989.) tensor(-1582722.8750)\n",
      "progress: 47 tensor(2.3732e+11)\n",
      "\tgrad:  1.0 2.0 tensor(182818.2656) tensor(182818.2656) tensor(182818.2656)\n",
      "\tgrad:  2.0 4.0 tensor(1360441.) tensor(2538063.7500) tensor(771629.6250)\n",
      "\tgrad:  3.0 6.0 tensor(5067482.5000) tensor(13659188.) tensor(2007310.1250)\n",
      "progress: 48 tensor(3.8173e+11)\n",
      "\tgrad:  1.0 2.0 tensor(-231861.3281) tensor(-231861.3281) tensor(-231861.3281)\n",
      "\tgrad:  2.0 4.0 tensor(-1725399.6250) tensor(-3218937.7500) tensor(-978630.4375)\n",
      "\tgrad:  3.0 6.0 tensor(-6426904.5000) tensor(-17323452.) tensor(-2545798.7500)\n",
      "progress: 49 tensor(6.1400e+11)\n",
      "\tgrad:  1.0 2.0 tensor(294061.7500) tensor(294061.7500) tensor(294061.7500)\n",
      "\tgrad:  2.0 4.0 tensor(2188260.) tensor(4082458.5000) tensor(1241161.)\n",
      "\tgrad:  3.0 6.0 tensor(8151009.) tensor(21970706.) tensor(3228744.)\n",
      "progress: 50 tensor(9.8762e+11)\n",
      "\tgrad:  1.0 2.0 tensor(-372947.4375) tensor(-372947.4375) tensor(-372947.4375)\n",
      "\tgrad:  2.0 4.0 tensor(-2775292.7500) tensor(-5177638.) tensor(-1574120.)\n",
      "\tgrad:  3.0 6.0 tensor(-10337631.) tensor(-27864654.) tensor(-4094899.5000)\n",
      "progress: 51 tensor(1.5886e+12)\n",
      "\tgrad:  1.0 2.0 tensor(472996.2500) tensor(472996.2500) tensor(472996.2500)\n",
      "\tgrad:  2.0 4.0 tensor(3519802.) tensor(6566608.) tensor(1996399.1250)\n",
      "\tgrad:  3.0 6.0 tensor(13110845.) tensor(35339736.) tensor(5193413.5000)\n",
      "progress: 52 tensor(2.5552e+12)\n",
      "\tgrad:  1.0 2.0 tensor(-599883.6250) tensor(-599883.6250) tensor(-599883.6250)\n",
      "\tgrad:  2.0 4.0 tensor(-4464039.5000) tensor(-8328195.) tensor(-2531961.5000)\n",
      "\tgrad:  3.0 6.0 tensor(-16628010.) tensor(-44820108.) tensor(-6586618.5000)\n",
      "progress: 53 tensor(4.1101e+12)\n",
      "\tgrad:  1.0 2.0 tensor(760811.0625) tensor(760811.0625) tensor(760811.0625)\n",
      "\tgrad:  2.0 4.0 tensor(5661578.) tensor(10562345.) tensor(3211194.5000)\n",
      "\tgrad:  3.0 6.0 tensor(21088704.) tensor(56843720.) tensor(8353569.5000)\n",
      "progress: 54 tensor(6.6110e+12)\n",
      "\tgrad:  1.0 2.0 tensor(-964908.7500) tensor(-964908.7500) tensor(-964908.7500)\n",
      "\tgrad:  2.0 4.0 tensor(-7180376.) tensor(-13395844.) tensor(-4072642.5000)\n",
      "\tgrad:  3.0 6.0 tensor(-26746042.) tensor(-72092840.) tensor(-10594531.)\n",
      "progress: 55 tensor(1.0634e+13)\n",
      "\tgrad:  1.0 2.0 tensor(1223759.5000) tensor(1223759.5000) tensor(1223759.5000)\n",
      "\tgrad:  2.0 4.0 tensor(9106611.) tensor(16989462.) tensor(5165185.)\n",
      "\tgrad:  3.0 6.0 tensor(33921036.) tensor(91432744.) tensor(13436660.)\n",
      "progress: 56 tensor(1.7104e+13)\n",
      "\tgrad:  1.0 2.0 tensor(-1552049.3750) tensor(-1552049.3750) tensor(-1552049.3750)\n",
      "\tgrad:  2.0 4.0 tensor(-11549586.) tensor(-21547124.) tensor(-6550818.)\n",
      "\tgrad:  3.0 6.0 tensor(-43020828.) tensor(-1.1596e+08) tensor(-17041232.)\n",
      "progress: 57 tensor(2.7512e+13)\n",
      "\tgrad:  1.0 2.0 tensor(1968408.8750) tensor(1968408.8750) tensor(1968408.8750)\n",
      "\tgrad:  2.0 4.0 tensor(14647923.) tensor(27327436.) tensor(8308166.)\n",
      "\tgrad:  3.0 6.0 tensor(54561764.) tensor(1.4707e+08) tensor(21612780.)\n",
      "progress: 58 tensor(4.4253e+13)\n",
      "\tgrad:  1.0 2.0 tensor(-2496461.2500) tensor(-2496461.2500) tensor(-2496461.2500)\n",
      "\tgrad:  2.0 4.0 tensor(-18577434.) tensor(-34658404.) tensor(-10536947.)\n",
      "\tgrad:  3.0 6.0 tensor(-69198712.) tensor(-1.8652e+08) tensor(-27410708.)\n",
      "progress: 59 tensor(7.1181e+13)\n",
      "\tgrad:  1.0 2.0 tensor(3166172.) tensor(3166172.) tensor(3166172.)\n",
      "\tgrad:  2.0 4.0 tensor(23561084.) tensor(43955996.) tensor(13363628.)\n",
      "\tgrad:  3.0 6.0 tensor(87762224.) tensor(2.3656e+08) tensor(34764008.)\n",
      "progress: 60 tensor(1.1449e+14)\n",
      "\tgrad:  1.0 2.0 tensor(-4015541.5000) tensor(-4015541.5000) tensor(-4015541.5000)\n",
      "\tgrad:  2.0 4.0 tensor(-29881678.) tensor(-55747812.) tensor(-16948610.)\n",
      "\tgrad:  3.0 6.0 tensor(-1.1131e+08) tensor(-3.0002e+08) tensor(-44089940.)\n",
      "progress: 61 tensor(1.8416e+14)\n",
      "\tgrad:  1.0 2.0 tensor(5092766.5000) tensor(5092766.5000) tensor(5092766.5000)\n",
      "\tgrad:  2.0 4.0 tensor(37897848.) tensor(70702928.) tensor(21495306.)\n",
      "\tgrad:  3.0 6.0 tensor(1.4116e+08) tensor(3.8050e+08) tensor(55917672.)\n",
      "progress: 62 tensor(2.9622e+14)\n",
      "\tgrad:  1.0 2.0 tensor(-6458969.) tensor(-6458969.) tensor(-6458969.)\n",
      "\tgrad:  2.0 4.0 tensor(-48064456.) tensor(-89669944.) tensor(-27261712.)\n",
      "\tgrad:  3.0 6.0 tensor(-1.7903e+08) tensor(-4.8258e+08) tensor(-70918344.)\n",
      "progress: 63 tensor(4.7648e+14)\n",
      "\tgrad:  1.0 2.0 tensor(8191678.) tensor(8191678.) tensor(8191678.)\n",
      "\tgrad:  2.0 4.0 tensor(60958416.) tensor(1.1373e+08) tensor(34575048.)\n",
      "\tgrad:  3.0 6.0 tensor(2.2706e+08) tensor(6.1204e+08) tensor(89943184.)\n",
      "progress: 64 tensor(7.6641e+14)\n",
      "\tgrad:  1.0 2.0 tensor(-10389208.) tensor(-10389208.) tensor(-10389208.)\n",
      "\tgrad:  2.0 4.0 tensor(-77311352.) tensor(-1.4423e+08) tensor(-43850280.)\n",
      "\tgrad:  3.0 6.0 tensor(-2.8798e+08) tensor(-7.7623e+08) tensor(-1.1407e+08)\n",
      "progress: 65 tensor(1.2328e+15)\n",
      "\tgrad:  1.0 2.0 tensor(13176260.) tensor(13176260.) tensor(13176260.)\n",
      "\tgrad:  2.0 4.0 tensor(98051216.) tensor(1.8293e+08) tensor(55613736.)\n",
      "\tgrad:  3.0 6.0 tensor(3.6523e+08) tensor(9.8446e+08) tensor(1.4467e+08)\n",
      "progress: 66 tensor(1.9829e+15)\n",
      "\tgrad:  1.0 2.0 tensor(-16710973.) tensor(-16710973.) tensor(-16710973.)\n",
      "\tgrad:  2.0 4.0 tensor(-1.2435e+08) tensor(-2.3200e+08) tensor(-70532888.)\n",
      "\tgrad:  3.0 6.0 tensor(-4.6321e+08) tensor(-1.2486e+09) tensor(-1.8348e+08)\n",
      "progress: 67 tensor(3.1895e+15)\n",
      "\tgrad:  1.0 2.0 tensor(21193924.) tensor(21193924.) tensor(21193924.)\n",
      "\tgrad:  2.0 4.0 tensor(1.5771e+08) tensor(2.9424e+08) tensor(89454320.)\n",
      "\tgrad:  3.0 6.0 tensor(5.8747e+08) tensor(1.5835e+09) tensor(2.3271e+08)\n",
      "progress: 68 tensor(5.1302e+15)\n",
      "\tgrad:  1.0 2.0 tensor(-26879488.) tensor(-26879488.) tensor(-26879488.)\n",
      "\tgrad:  2.0 4.0 tensor(-2.0002e+08) tensor(-3.7317e+08) tensor(-1.1345e+08)\n",
      "\tgrad:  3.0 6.0 tensor(-7.4506e+08) tensor(-2.0083e+09) tensor(-2.9513e+08)\n",
      "progress: 69 tensor(8.2519e+15)\n",
      "\tgrad:  1.0 2.0 tensor(34090284.) tensor(34090284.) tensor(34090284.)\n",
      "\tgrad:  2.0 4.0 tensor(2.5368e+08) tensor(4.7328e+08) tensor(1.4389e+08)\n",
      "\tgrad:  3.0 6.0 tensor(9.4494e+08) tensor(2.5470e+09) tensor(3.7431e+08)\n",
      "progress: 70 tensor(1.3273e+16)\n",
      "\tgrad:  1.0 2.0 tensor(-43235476.) tensor(-43235476.) tensor(-43235476.)\n",
      "\tgrad:  2.0 4.0 tensor(-3.2174e+08) tensor(-6.0024e+08) tensor(-1.8249e+08)\n",
      "\tgrad:  3.0 6.0 tensor(-1.1984e+09) tensor(-3.2303e+09) tensor(-4.7472e+08)\n",
      "progress: 71 tensor(2.1350e+16)\n",
      "\tgrad:  1.0 2.0 tensor(54833988.) tensor(54833988.) tensor(54833988.)\n",
      "\tgrad:  2.0 4.0 tensor(4.0805e+08) tensor(7.6126e+08) tensor(2.3144e+08)\n",
      "\tgrad:  3.0 6.0 tensor(1.5199e+09) tensor(4.0969e+09) tensor(6.0207e+08)\n",
      "progress: 72 tensor(3.4341e+16)\n",
      "\tgrad:  1.0 2.0 tensor(-69543952.) tensor(-69543952.) tensor(-69543952.)\n",
      "\tgrad:  2.0 4.0 tensor(-5.1751e+08) tensor(-9.6548e+08) tensor(-2.9353e+08)\n",
      "\tgrad:  3.0 6.0 tensor(-1.9277e+09) tensor(-5.1960e+09) tensor(-7.6358e+08)\n",
      "progress: 73 tensor(5.5237e+16)\n",
      "\tgrad:  1.0 2.0 tensor(88200080.) tensor(88200080.) tensor(88200080.)\n",
      "\tgrad:  2.0 4.0 tensor(6.5634e+08) tensor(1.2245e+09) tensor(3.7227e+08)\n",
      "\tgrad:  3.0 6.0 tensor(2.4448e+09) tensor(6.5898e+09) tensor(9.6842e+08)\n",
      "progress: 74 tensor(8.8849e+16)\n",
      "\tgrad:  1.0 2.0 tensor(-1.1186e+08) tensor(-1.1186e+08) tensor(-1.1186e+08)\n",
      "\tgrad:  2.0 4.0 tensor(-8.3241e+08) tensor(-1.5530e+09) tensor(-4.7214e+08)\n",
      "\tgrad:  3.0 6.0 tensor(-3.1006e+09) tensor(-8.3577e+09) tensor(-1.2282e+09)\n",
      "progress: 75 tensor(1.4291e+17)\n",
      "\tgrad:  1.0 2.0 tensor(1.4187e+08) tensor(1.4187e+08) tensor(1.4187e+08)\n",
      "\tgrad:  2.0 4.0 tensor(1.0557e+09) tensor(1.9696e+09) tensor(5.9880e+08)\n",
      "\tgrad:  3.0 6.0 tensor(3.9324e+09) tensor(1.0600e+10) tensor(1.5577e+09)\n",
      "progress: 76 tensor(2.2987e+17)\n",
      "\tgrad:  1.0 2.0 tensor(-1.7993e+08) tensor(-1.7993e+08) tensor(-1.7993e+08)\n",
      "\tgrad:  2.0 4.0 tensor(-1.3389e+09) tensor(-2.4979e+09) tensor(-7.5943e+08)\n",
      "\tgrad:  3.0 6.0 tensor(-4.9874e+09) tensor(-1.3443e+10) tensor(-1.9756e+09)\n",
      "progress: 77 tensor(3.6975e+17)\n",
      "\tgrad:  1.0 2.0 tensor(2.2820e+08) tensor(2.2820e+08) tensor(2.2820e+08)\n",
      "\tgrad:  2.0 4.0 tensor(1.6981e+09) tensor(3.1680e+09) tensor(9.6316e+08)\n",
      "\tgrad:  3.0 6.0 tensor(6.3253e+09) tensor(1.7050e+10) tensor(2.5055e+09)\n",
      "progress: 78 tensor(5.9474e+17)\n",
      "\tgrad:  1.0 2.0 tensor(-2.8941e+08) tensor(-2.8941e+08) tensor(-2.8941e+08)\n",
      "\tgrad:  2.0 4.0 tensor(-2.1537e+09) tensor(-4.0179e+09) tensor(-1.2215e+09)\n",
      "\tgrad:  3.0 6.0 tensor(-8.0221e+09) tensor(-2.1623e+10) tensor(-3.1777e+09)\n",
      "progress: 79 tensor(9.5664e+17)\n",
      "\tgrad:  1.0 2.0 tensor(3.6705e+08) tensor(3.6705e+08) tensor(3.6705e+08)\n",
      "\tgrad:  2.0 4.0 tensor(2.7314e+09) tensor(5.0958e+09) tensor(1.5492e+09)\n",
      "\tgrad:  3.0 6.0 tensor(1.0174e+10) tensor(2.7424e+10) tensor(4.0302e+09)\n",
      "progress: 80 tensor(1.5387e+18)\n",
      "\tgrad:  1.0 2.0 tensor(-4.6552e+08) tensor(-4.6552e+08) tensor(-4.6552e+08)\n",
      "\tgrad:  2.0 4.0 tensor(-3.4642e+09) tensor(-6.4628e+09) tensor(-1.9648e+09)\n",
      "\tgrad:  3.0 6.0 tensor(-1.2904e+10) tensor(-3.4781e+10) tensor(-5.1113e+09)\n",
      "progress: 81 tensor(2.4751e+18)\n",
      "\tgrad:  1.0 2.0 tensor(5.9040e+08) tensor(5.9040e+08) tensor(5.9040e+08)\n",
      "\tgrad:  2.0 4.0 tensor(4.3935e+09) tensor(8.1965e+09) tensor(2.4919e+09)\n",
      "\tgrad:  3.0 6.0 tensor(1.6365e+10) tensor(4.4111e+10) tensor(6.4825e+09)\n",
      "progress: 82 tensor(3.9811e+18)\n",
      "\tgrad:  1.0 2.0 tensor(-7.4878e+08) tensor(-7.4878e+08) tensor(-7.4878e+08)\n",
      "\tgrad:  2.0 4.0 tensor(-5.5721e+09) tensor(-1.0395e+10) tensor(-3.1604e+09)\n",
      "\tgrad:  3.0 6.0 tensor(-2.0755e+10) tensor(-5.5945e+10) tensor(-8.2215e+09)\n",
      "progress: 83 tensor(6.4036e+18)\n",
      "\tgrad:  1.0 2.0 tensor(9.4965e+08) tensor(9.4965e+08) tensor(9.4965e+08)\n",
      "\tgrad:  2.0 4.0 tensor(7.0669e+09) tensor(1.3184e+10) tensor(4.0083e+09)\n",
      "\tgrad:  3.0 6.0 tensor(2.6323e+10) tensor(7.0953e+10) tensor(1.0427e+10)\n",
      "progress: 84 tensor(1.0300e+19)\n",
      "\tgrad:  1.0 2.0 tensor(-1.2044e+09) tensor(-1.2044e+09) tensor(-1.2044e+09)\n",
      "\tgrad:  2.0 4.0 tensor(-8.9626e+09) tensor(-1.6721e+10) tensor(-5.0835e+09)\n",
      "\tgrad:  3.0 6.0 tensor(-3.3385e+10) tensor(-8.9987e+10) tensor(-1.3224e+10)\n",
      "progress: 85 tensor(1.6568e+19)\n",
      "\tgrad:  1.0 2.0 tensor(1.5275e+09) tensor(1.5275e+09) tensor(1.5275e+09)\n",
      "\tgrad:  2.0 4.0 tensor(1.1367e+10) tensor(2.1206e+10) tensor(6.4472e+09)\n",
      "\tgrad:  3.0 6.0 tensor(4.2341e+10) tensor(1.1413e+11) tensor(1.6772e+10)\n",
      "progress: 86 tensor(2.6649e+19)\n",
      "\tgrad:  1.0 2.0 tensor(-1.9373e+09) tensor(-1.9373e+09) tensor(-1.9373e+09)\n",
      "\tgrad:  2.0 4.0 tensor(-1.4416e+10) tensor(-2.6895e+10) tensor(-8.1768e+09)\n",
      "\tgrad:  3.0 6.0 tensor(-5.3699e+10) tensor(-1.4474e+11) tensor(-2.1271e+10)\n",
      "progress: 87 tensor(4.2865e+19)\n",
      "\tgrad:  1.0 2.0 tensor(2.4570e+09) tensor(2.4570e+09) tensor(2.4570e+09)\n",
      "\tgrad:  2.0 4.0 tensor(1.8284e+10) tensor(3.4110e+10) tensor(1.0370e+10)\n",
      "\tgrad:  3.0 6.0 tensor(6.8105e+10) tensor(1.8357e+11) tensor(2.6977e+10)\n",
      "progress: 88 tensor(6.8948e+19)\n",
      "\tgrad:  1.0 2.0 tensor(-3.1161e+09) tensor(-3.1161e+09) tensor(-3.1161e+09)\n",
      "\tgrad:  2.0 4.0 tensor(-2.3189e+10) tensor(-4.3261e+10) tensor(-1.3152e+10)\n",
      "\tgrad:  3.0 6.0 tensor(-8.6375e+10) tensor(-2.3282e+11) tensor(-3.4214e+10)\n",
      "progress: 89 tensor(1.1090e+20)\n",
      "\tgrad:  1.0 2.0 tensor(3.9521e+09) tensor(3.9521e+09) tensor(3.9521e+09)\n",
      "\tgrad:  2.0 4.0 tensor(2.9409e+10) tensor(5.4866e+10) tensor(1.6681e+10)\n",
      "\tgrad:  3.0 6.0 tensor(1.0955e+11) tensor(2.9528e+11) tensor(4.3393e+10)\n",
      "progress: 90 tensor(1.7839e+20)\n",
      "\tgrad:  1.0 2.0 tensor(-5.0122e+09) tensor(-5.0122e+09) tensor(-5.0122e+09)\n",
      "\tgrad:  2.0 4.0 tensor(-3.7299e+10) tensor(-6.9585e+10) tensor(-2.1155e+10)\n",
      "\tgrad:  3.0 6.0 tensor(-1.3893e+11) tensor(-3.7449e+11) tensor(-5.5034e+10)\n",
      "progress: 91 tensor(2.8693e+20)\n",
      "\tgrad:  1.0 2.0 tensor(6.3569e+09) tensor(6.3569e+09) tensor(6.3569e+09)\n",
      "\tgrad:  2.0 4.0 tensor(4.7305e+10) tensor(8.8252e+10) tensor(2.6831e+10)\n",
      "\tgrad:  3.0 6.0 tensor(1.7620e+11) tensor(4.7495e+11) tensor(6.9797e+10)\n",
      "progress: 92 tensor(4.6153e+20)\n",
      "\tgrad:  1.0 2.0 tensor(-8.0622e+09) tensor(-8.0622e+09) tensor(-8.0622e+09)\n",
      "\tgrad:  2.0 4.0 tensor(-5.9995e+10) tensor(-1.1193e+11) tensor(-3.4028e+10)\n",
      "\tgrad:  3.0 6.0 tensor(-2.2347e+11) tensor(-6.0236e+11) tensor(-8.8521e+10)\n",
      "progress: 93 tensor(7.4237e+20)\n",
      "\tgrad:  1.0 2.0 tensor(1.0225e+10) tensor(1.0225e+10) tensor(1.0225e+10)\n",
      "\tgrad:  2.0 4.0 tensor(7.6089e+10) tensor(1.4195e+11) tensor(4.3157e+10)\n",
      "\tgrad:  3.0 6.0 tensor(2.8342e+11) tensor(7.6395e+11) tensor(1.1227e+11)\n",
      "progress: 94 tensor(1.1941e+21)\n",
      "\tgrad:  1.0 2.0 tensor(-1.2968e+10) tensor(-1.2968e+10) tensor(-1.2968e+10)\n",
      "\tgrad:  2.0 4.0 tensor(-9.6501e+10) tensor(-1.8003e+11) tensor(-5.4734e+10)\n",
      "\tgrad:  3.0 6.0 tensor(-3.5945e+11) tensor(-9.6890e+11) tensor(-1.4239e+11)\n",
      "progress: 95 tensor(1.9207e+21)\n",
      "\tgrad:  1.0 2.0 tensor(1.6447e+10) tensor(1.6447e+10) tensor(1.6447e+10)\n",
      "\tgrad:  2.0 4.0 tensor(1.2239e+11) tensor(2.2833e+11) tensor(6.9418e+10)\n",
      "\tgrad:  3.0 6.0 tensor(4.5588e+11) tensor(1.2288e+12) tensor(1.8058e+11)\n",
      "progress: 96 tensor(3.0894e+21)\n",
      "\tgrad:  1.0 2.0 tensor(-2.0859e+10) tensor(-2.0859e+10) tensor(-2.0859e+10)\n",
      "\tgrad:  2.0 4.0 tensor(-1.5522e+11) tensor(-2.8958e+11) tensor(-8.8040e+10)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7818e+11) tensor(-1.5585e+12) tensor(-2.2903e+11)\n",
      "progress: 97 tensor(4.9693e+21)\n",
      "\tgrad:  1.0 2.0 tensor(2.6455e+10) tensor(2.6455e+10) tensor(2.6455e+10)\n",
      "\tgrad:  2.0 4.0 tensor(1.9686e+11) tensor(3.6727e+11) tensor(1.1166e+11)\n",
      "\tgrad:  3.0 6.0 tensor(7.3329e+11) tensor(1.9765e+12) tensor(2.9047e+11)\n",
      "progress: 98 tensor(7.9931e+21)\n",
      "\tgrad:  1.0 2.0 tensor(-3.3551e+10) tensor(-3.3551e+10) tensor(-3.3551e+10)\n",
      "\tgrad:  2.0 4.0 tensor(-2.4967e+11) tensor(-4.6579e+11) tensor(-1.4161e+11)\n",
      "\tgrad:  3.0 6.0 tensor(-9.3000e+11) tensor(-2.5068e+12) tensor(-3.6839e+11)\n",
      "progress: 99 tensor(1.2857e+22)\n",
      "predict (after training) 4 tensor(2.4712e+11)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w1 = torch.tensor([1.0],  requires_grad=True)  # Any random value\n",
    "w2 = torch.tensor([1.0],  requires_grad=True) \n",
    "b = torch.tensor([1.0],  requires_grad=True) \n",
    "# our model forward pass\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return x ** 2 * w2 + x * w1 + b\n",
    "\n",
    "# Loss function\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\", 4, forward(4).data[0])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, y_val)\n",
    "        l.backward()\n",
    "        print(\"\\tgrad: \", x_val, y_val, w1.grad.data[0], w2.grad.data[0], b.grad.data[0])\n",
    "    w1.data = w1.data - 0.01 * w1.grad.data\n",
    "    w2.data = w2.data - 0.01 * w2.grad.data\n",
    "    b.data = b.data - 0.01 * b.grad.data\n",
    "    # Manually zero the gradients after updating weights\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "    print(\"progress:\", epoch, l.data[0])\n",
    "\n",
    "# After training\n",
    "print(\"predict (after training)\", 4, forward(4).data[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
