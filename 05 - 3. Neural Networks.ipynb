{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Neural Networks\n",
    "===============\n",
    "\n",
    "Neural networks can be constructed using the ``torch.nn`` package.\n",
    "\n",
    "It is a simple feed-forward network. It takes the input, feeds it\n",
    "through several layers one after the other, and then finally gives the\n",
    "output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.tensor([[2.0], [4.0], [6.0]])\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out, linear = y=wx + b\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# our model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9441], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1)\n",
    "out = model(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero the gradient buffers of all parameters and backprops with random\n",
    "gradients:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x_data)\n",
    "\n",
    "# Compute and print loss\n",
    "loss = criterion(y_pred, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7f34528bc860>\n",
      "<AddmmBackward object at 0x7f34528bc898>\n",
      "<AccumulateGrad object at 0x7f34528bc860>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backprop\n",
    "--------\n",
    "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
    "You need to clear the existing gradients though, else gradients will be\n",
    "accumulated to existing gradients.\n",
    "\n",
    "\n",
    "Now we shall call ``loss.backward()``, and have a look at conv1's bias\n",
    "gradients before and after the backward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 44.83409881591797\n",
      "1 19.98935890197754\n",
      "2 8.928750038146973\n",
      "3 4.0044474601745605\n",
      "4 1.8118624687194824\n",
      "5 0.8353658318519592\n",
      "6 0.4002443850040436\n",
      "7 0.20613279938697815\n",
      "8 0.11931825429201126\n",
      "9 0.08027452975511551\n",
      "10 0.06250318884849548\n",
      "11 0.05420708656311035\n",
      "12 0.0501348115503788\n",
      "13 0.04794839769601822\n",
      "14 0.04660667106509209\n",
      "15 0.0456463024020195\n",
      "16 0.044860996305942535\n",
      "17 0.04415861517190933\n",
      "18 0.04349828138947487\n",
      "19 0.0428616888821125\n",
      "20 0.0422406904399395\n",
      "21 0.041631270200014114\n",
      "22 0.04103211313486099\n",
      "23 0.040441811084747314\n",
      "24 0.03986046835780144\n",
      "25 0.03928754851222038\n",
      "26 0.0387229286134243\n",
      "27 0.03816642239689827\n",
      "28 0.037617865949869156\n",
      "29 0.037077274173498154\n",
      "30 0.0365443117916584\n",
      "31 0.03601912036538124\n",
      "32 0.03550156578421593\n",
      "33 0.034991227090358734\n",
      "34 0.03448837250471115\n",
      "35 0.033992670476436615\n",
      "36 0.03350421041250229\n",
      "37 0.03302275016903877\n",
      "38 0.032548122107982635\n",
      "39 0.03208036348223686\n",
      "40 0.03161931037902832\n",
      "41 0.0311648678034544\n",
      "42 0.0307170283049345\n",
      "43 0.03027549386024475\n",
      "44 0.02984047122299671\n",
      "45 0.029411587864160538\n",
      "46 0.028988877311348915\n",
      "47 0.028572363778948784\n",
      "48 0.028161652386188507\n",
      "49 0.027756918221712112\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(50):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (after training) 5 tensor([28.5885], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# After training\n",
    "hour_var = torch.tensor([[4.0]])\n",
    "y_pred = model(hour_var)\n",
    "print(\"predict (after training)\",  15, model(torch.tensor([15.0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
